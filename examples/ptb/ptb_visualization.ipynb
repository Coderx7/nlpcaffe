{
 "metadata": {
  "description": "Extracting features and visualizing trained filters with an example image, viewed layer-by-layer.",
  "example_name": "Filter visualization",
  "include_in_docs": true,
  "priority": 2,
  "signature": "sha256:06ae6ae2146652c917a7d5154e59a288cbfc4a82b45132d0fc557829f541635e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import numpy as np\n",
      "import pickle\n",
      "from tsne import bh_sne\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "import lmdb\n",
      "import itertools\n",
      "\n",
      "\n",
      "import mpld3\n",
      "mpld3.enable_notebook()\n",
      "from mpld3 import plugins\n",
      "\n",
      "\n",
      "# Make sure that caffe is on the python path:\n",
      "caffe_root = '../'  # this file is expected to be in {caffe_root}/examples\n",
      "import sys\n",
      "sys.path.insert(0, caffe_root + 'python')\n",
      "sys.path.insert(0, caffe_root + 'python/caffe/proto')\n",
      "sys.path.insert(0, caffe_root + '/models/rnn')\n",
      "sys.path.insert(0, caffe_root)\n",
      "import caffe_pb2\n",
      "import caffe\n",
      "import mono_mkproto as mkproto"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_id = 1\n",
      "base_param = mkproto.get_base_param(model_id)\n",
      "model_folder = 'caffe_merge'\n",
      "caffe.set_mode_gpu()\n",
      "net = caffe.Classifier(caffe_root + 'models/rnn/deploy%d.prototxt' % model_id,\n",
      "                       '/snapshots/%s.%d_iter_20000.caffemodel' % (model_folder, model_id))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = pickle.load(open('/home/stewartr/data/penn/vocab.pkl', 'r'))\n",
      "inv = lambda d: {v:k for k,v in d.iteritems()}\n",
      "vocab_inv = inv(vocab)\n",
      "\n",
      "def get_datums():\n",
      "    env = lmdb.open(caffe_root + '/models/rnn/rnn_train_db')\n",
      "    with env.begin() as txn:\n",
      "        cursor = txn.cursor()\n",
      "        while cursor.next():\n",
      "            datum = caffe_pb2.Datum()\n",
      "            datum.ParseFromString(cursor.value())\n",
      "            yield datum\n",
      "\n",
      "#datums = list(itertools.islice(get_datums(), 3000))\n",
      "datums = list(get_datums())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(datums):\n",
      "    n = datums[0].channels // 2\n",
      "    for i in range(0, len(datums), base_param['deploy_batch_size']):\n",
      "        data = np.zeros((base_param['deploy_batch_size'], 2 * n, 1, 1))\n",
      "        for j in range(base_param['deploy_batch_size']):\n",
      "            if (i + j) >= len(datums):\n",
      "                return\n",
      "            data[j, :, 0, 0] = datums[i + j].float_data\n",
      "        yield data\n",
      "\n",
      "data_batches = list(get_data(datums))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score_sentence(datum, net, net_id):\n",
      "    n = datum.channels // 2\n",
      "    global_loss = np.zeros(n,)\n",
      "    local_loss = np.zeros(n,)\n",
      "    length = 0\n",
      "    for i in range(n):\n",
      "        #target_category_id = int(datum.float_data[n + i] + 0.5)\n",
      "        #target_local_id = int(datum.float_data[2 * n + i] + 0.5)\n",
      "\n",
      "        target_category_id = int(net.blobs['target_category_id'].data[net_id + i * base_param['deploy_batch_size']].flatten()[0])\n",
      "        target_local_id = int(net.blobs['target_local_id'].data[net_id + i * base_param['deploy_batch_size']].flatten()[0])\n",
      "        #assert target_category_id == mkproto.get_global_id(target, base_param['num_categories'])\n",
      "        #assert target_local_id == mkproto.get_local_id(target, base_param['num_categories'])\n",
      "        length += 1\n",
      "        if target_category_id == 0:\n",
      "            break\n",
      "        global_loss[i] = -math.log(net.blobs['category_prob'].data[net_id + base_param['deploy_batch_size'] * i, target_category_id])\n",
      "        local_loss[i] = -math.log(net.blobs['local_prob'].data[net_id + base_param['deploy_batch_size'] * i, target_local_id])\n",
      "\n",
      "    return local_loss, global_loss, length"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "global_losses = []\n",
      "local_losses = []\n",
      "lengths = []\n",
      "for i, batch in enumerate(data_batches):\n",
      "    net.rff(batch)\n",
      "    break\n",
      "    for j in range(len(batch)):\n",
      "        datum = datums[j + base_param['deploy_batch_size'] * i] \n",
      "        local_loss, global_loss, length = score_sentence(datum, net, j)\n",
      "        global_losses.append(global_loss)\n",
      "        local_losses.append(local_loss)\n",
      "        \n",
      "        lengths.append(length)\n",
      "        #print (global_loss , local_loss) #/ length\n",
      "    print '%d' % i\n",
      "    print 'local: %s' % (np.sum(local_losses) / sum(lengths))\n",
      "    print 'global: %s' % (np.sum(global_losses) / sum(lengths))\n",
      "    break\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "OrderedDict()"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(net.params['target_wordvec_layer'][0].data[0,-1,0, :])\n",
      "(net.params['target_wordvec_layer'][0].data.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'target_wordvec_layer'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-29-38f437ba560c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_wordvec_layer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_wordvec_layer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: 'target_wordvec_layer'"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_vectors = {k:net.params['target_wordvec_layer'][0].data[0, vocab[k] + 1, 0, :] for (k, v) in vocab.items() if v < base_param['target_vocab_size']}\n",
      "print sorted(word_vectors.keys(), key= lambda x: np.linalg.norm(word_vectors['three'] - word_vectors[x]))[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OrderedDict()\n"
       ]
      },
      {
       "ename": "KeyError",
       "evalue": "'target_wordvec_layer'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-bdcf6c9ad383>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_wordvec_layer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbase_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_vocab_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'three'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-12-bdcf6c9ad383>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m((k, v))\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_wordvec_layer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbase_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_vocab_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'three'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: 'target_wordvec_layer'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = np.zeros((len(word_vectors), len(word_vectors.values()[0].flatten())))\n",
      "for k, v in word_vectors.items():\n",
      "    M[vocab[k], :] = v\n",
      "    \n",
      "X_2d = bh_sne(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-51-c61320836538>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbh_sne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tsne/__init__.pyc\u001b[0m in \u001b[0;36mbh_sne\u001b[1;34m(data, pca_d, d, perplexity, theta)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBH_SNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(8,8))\n",
      "ax.grid(color='white', linestyle='solid')\n",
      "\n",
      "num_words_to_plot = 100\n",
      "\n",
      "scatter = ax.scatter(X_2d[:num_words_to_plot, 0], X_2d[:num_words_to_plot, 1], s = 100, c='#3EEEEE', alpha=0.1, cmap=plt.cm.jet)\n",
      "labels = [vocab_inv[i] for i in range(num_words_to_plot)]\n",
      "plugins.connect(fig, plugins.PointLabelTooltip(scatter, labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab['six']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "362"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}