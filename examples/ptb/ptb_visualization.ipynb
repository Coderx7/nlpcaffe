{
 "metadata": {
  "description": "Extracting features and visualizing trained filters with an example image, viewed layer-by-layer.",
  "example_name": "Filter visualization",
  "include_in_docs": true,
  "priority": 2,
  "signature": "sha256:d7159217583e8a280e5998a33ddb01a3405a527de87de7999946435fcc705a89"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook requires the user to have installed tsne and mpld3 with:\n",
      "    \n",
      "    pip install mpld3\n",
      "    pip install tsne"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import numpy as np\n",
      "import pickle\n",
      "from tsne import bh_sne\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "import lmdb\n",
      "import itertools\n",
      "\n",
      "import mpld3\n",
      "mpld3.enable_notebook()\n",
      "from mpld3 import plugins\n",
      "\n",
      "# Make sure that caffe is on the python path:\n",
      "caffe_root = '../../'  # this file is expected to be in {caffe_root}/examples/ptb\n",
      "import sys\n",
      "sys.path.insert(0, caffe_root + 'python')\n",
      "import caffe\n",
      "sys.path.insert(0, caffe_root + 'python/caffe/proto')\n",
      "import caffe_pb2\n",
      "sys.path.insert(0, caffe_root + 'examples/ptb')\n",
      "import create_ptb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "base_param = create_ptb.get_base_param()\n",
      "model_folder = 'caffe_merge'\n",
      "caffe.set_mode_gpu()\n",
      "net = caffe.Classifier(caffe_root + 'examples/ptb/deploy.prototxt',\n",
      "                       caffe_root + 'examples/ptb/ptb_iter_10000.caffemodel')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_datums():\n",
      "    env = lmdb.open(caffe_root + 'examples/ptb/ptb_test_db')\n",
      "    with env.begin() as txn:\n",
      "        cursor = txn.cursor()\n",
      "        while cursor.next():\n",
      "            datum = caffe_pb2.Datum()\n",
      "            datum.ParseFromString(cursor.value())\n",
      "            yield datum"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fast_test = False\n",
      "if fast_test:   \n",
      "    datums = list(itertools.islice(get_datums(), 1000))\n",
      "else:\n",
      "    datums = list(get_datums())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(datums):\n",
      "    n = datums[0].channels // 2\n",
      "    for i in range(0, len(datums), base_param['deploy_batch_size']):\n",
      "        data = np.zeros((base_param['deploy_batch_size'], 2 * n, 1, 1))\n",
      "        for j in range(base_param['deploy_batch_size']):\n",
      "            if (i + j) >= len(datums):\n",
      "                return\n",
      "            data[j, :, 0, 0] = datums[i + j].float_data\n",
      "        yield data\n",
      "\n",
      "data_batches = list(get_data(datums))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score_sentence(datum, net, net_id):\n",
      "    n = datum.channels // 2\n",
      "    loss = np.zeros(n,)\n",
      "    length = 0\n",
      "    for i in range(n):\n",
      "        target_category_id = int(net.blobs['target_words'].data[net_id, i].flatten()[0])\n",
      "        if target_category_id == base_param['zero_symbol']:\n",
      "            break\n",
      "        length += 1\n",
      "        loss[i] = -math.log(net.blobs['word_probs'].data[net_id + base_param['deploy_batch_size'] * i, target_category_id])\n",
      "\n",
      "    return loss, length"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "losses = []\n",
      "lengths = []\n",
      "for i, batch in enumerate(data_batches):\n",
      "    net.rff(batch)\n",
      "    for j in range(len(batch)):\n",
      "        datum = datums[j + base_param['deploy_batch_size'] * i] \n",
      "        loss, length = score_sentence(datum, net, j)\n",
      "        losses.append(loss)        \n",
      "        lengths.append(length)\n",
      "    if i % 5 == 0: print 'current loss: %s' % (np.sum(losses) / sum(lengths))\n",
      "print 'Final perplexity %s' % math.exp(np.sum(losses) / sum(lengths))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "current loss: 26.7226729617\n",
        "current loss: 27.3519478444"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-c156c918fe0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mdatum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbase_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deploy_batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-6-7318447e1403>\u001b[0m in \u001b[0;36mscore_sentence\u001b[1;34m(datum, net, net_id)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_probs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnet_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbase_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deploy_batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_category_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/stewartr/git/caffe_house/nlp_caffe/python/caffe/pycaffe.pyc\u001b[0m in \u001b[0;36m_Net_blobs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mblobs\u001b[0m \u001b[0mindexed\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blob_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mroot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_setitem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/_abcoll.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value, dict_setitem)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_setitem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;34m'od.__setitem__(i, y) <==> od[i]=y'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# Setting a new item creates a new link at the end of the linked list,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = pickle.load(open(caffe_root + 'data/ptb/vocab.pkl', 'r'))\n",
      "inv = lambda d: {v:k for k,v in d.iteritems()}\n",
      "vocab_inv = inv(vocab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_vectors = {k:net.params['wordvec_layer'][0].data[0, vocab[k], 0, :] for (k, v) in vocab.items() if v < base_param['vocab_size']}\n",
      "print sorted(word_vectors.keys(), key= lambda x: np.linalg.norm(word_vectors['four'] - word_vectors[x]))[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = np.zeros((len(word_vectors), len(word_vectors.values()[0].flatten())))\n",
      "for k, v in word_vectors.items():\n",
      "    M[vocab[k], :] = v\n",
      "    \n",
      "X_2d = bh_sne(M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(8,8))\n",
      "ax.grid(color='white', linestyle='solid')\n",
      "\n",
      "num_words_to_plot = 500\n",
      "\n",
      "scatter = ax.scatter(X_2d[:num_words_to_plot, 0], X_2d[:num_words_to_plot, 1], s = 100, c='#3EEEEE', alpha=0.1, cmap=plt.cm.jet)\n",
      "labels = [vocab_inv[i] for i in range(num_words_to_plot)]\n",
      "plugins.connect(fig, plugins.PointLabelTooltip(scatter, labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}